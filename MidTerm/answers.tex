\documentclass[a4paper, 10pt]{article}
\usepackage[margin=1.5in]{geometry}

\usepackage{blindtext}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{float}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{comment}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs,lipsum}

\graphicspath{ {../outputs/}{../data/} }


\setlength{\columnsep}{1cm}
\title{ENPM 690: Robot Learning}
\author{Aswath Muthuselvam \\ 118286204 \\aswath@umd.edu}
\date{10th March 2022}

\begin{document}
\maketitle
\newlist{contract}{enumerate}{10}
\setlist[contract]{label*=\arabic*.}
\setlistdepth{10} 

\textbf{1. Describe at least 5 differences between Local and Global Learning. Identify 3 methods for both, respectively.}

\textbf{Answer:}
Local learning occurs with an update of weights corresponding to an input window. Examples of local learning techniques include CMAC(Cerebellar Model Articulation Controller), RBF(Radial basis function), LWR(Locally weighted Regression), Gabriel Graph and SVM(Support Vector Machines). On the other hand, Global Learning methods involve setting a distribution over the data to summarize the whole mode. Examples of Global Learning include Conditional Learning, Maximum Likelihood learning, Bayesian Point Machine, and approaches like Bayesian Networks, Gaussian Mixture Models, and Hidden Markov Model.\cite{localglobal}

Some of the key differences between the two is that, first, local learning converges faster. Second, it is computationally inexpensive as it requires only local update of weights. Third, however, the drawback of local update feature is that it requires a lot more memory to store the local data than compressed interconnected representation of Global Learning method like Neural networks. Fourth, Local learning has very high accuracy as it maps the function very closely. However, in Global Learning, the model updates all of its weights to learn the function, due to this, there exists unpredictable local minima. Fifth, Global Learning can be considered as generative learning as it involves setting a hypothesis, such as a Gaussian distribution, and after receiving data, the hypothesis is updated to fine tune the model. 

\hfill

\textbf{2. Describe at least 5 differences between Lazy and Eager Learning. Identify 3 methods for both respectively.}

\textbf{Answer:}
First, Lazy Learning or Instance based learning methods use parts of the data that is requested as a vector input to train that part of the model. In other words, the function will be learnt locally, given an input data point. Second, due to the learning being as a fragmented process, changes in each of the classes or domains can be learnt parallely. Third, lots of memory is used while training the model. Fourth, due to it's nature, Lazy learning can be done in online manner. Because function is learnt locally, it can be sensitive to noise based on the hyper-parameter setting. Fifth, it starts classifying when it receives Test data. Sixth, it takes less time to learn and more time classifying. Lazy learning depends on choosing a good representation of features prior to the learning process, in KNN if the chosen mean is closer to the actual convergent mean, the learning will be faster. One-shot learning is supported by Lazy learning, with a very few datapoints, a good approximation of the function can be learnt.
 Examples of Lazy Learning include KNN(K-Nearest Neighbors), Locally Weighted Regression and Case-based Reasoning. 

Eager Learning approximates the global function before receiving a query. Second, It is limited to the global approximation to the target mapping. It learns the function globally during training. Third, extra memory is not used to store the intermediate results of a local function. Fourth, globally learning also means more robustness to the noise present in the data, that is, more data is available for the model at the same time to generalize. Fifth, Eager learning is done in offline manner. Sixth, it takes long time training and less time predicting.
Examples of Eager Learning includes Multilayer perceptrons, RBFs, Decision Trees, CMAC.

\hfill

\textbf{3. Explain the origins of the CMAC architecture for Machine Learning.}

\textbf{Answer:} 
CMAC is an architecture proposed by Albus in 1975. CMAC took inspiration from the study about the functions of cerebellum in the brain and how it performs motor functions and the role of various fibers that pass information from the sensory parts to the motors. The observation is that, the mathematical calculation the inverse Jacobian for acutuating an N degree of freedom arm is intractable as N becomes arbitrarily large, however, this calculation is trivial for even a small sized brains of animals which performs complex dynamic tasks like squirrels. It is observed that, the each input form the arm's sensory cell is mapped to a memory address and the corresponding weight(s) at that location changes the magnitude of the signal and outputs the next point in the trajectory for the arm to move.

Later, various works on CMAC incorporated with popular machine learning algorithms have been released and a short note on it is provided below.

Support Vector Machines(SVM) are supervised Machine Learning models used for data classification. SVM maps the data to space where it can seperate the margins between the classes. The mapping of this space is done with a help of a kernel function. In 2003, work by Gábor Horváth\cite{svmCMAC} used CMAC to learn this kernel function, the CMAC is thought of as a B-Spline kernel function. His work also explains why two dimensional CMAC kernels perform poorer than one dimension CMAC.

Gábor has also worked on CMAC based Neural Network\cite{kernelCMAC}, with the advantages faster learning capabilities and more efficient hardware implementations than MLP(Multilayer Perceptron) based Neural Networks. The kernel CMAC is benchmarked on 1D and 2D sinc functions for testing its approximation capabilities and on two spiral problem for its classification capabilities.

Work on CMAC based Q-Learning or Reinforcement Learning has been done by Yuan-Pao Hsu et al.\cite{qCMAC}, where the state of the sensors attached on a differential drive robot is passed to a conceptual memory prior to passing it to the CMAC to perform the actions of rotating the motors.

In 1995, Tham\cite{reinforcemetCMAC} incorporated CMAC in Hierarchical Mixture of Experts, the resulting architecture is used for implementing Compositional Q-Learning Framework.


\hfill

\textbf{4. Identify and describe at least 3 key features, 2 advantages, and 2 limitations of CMAC for Machine Learning.}

\textbf{Answer:}
CMAC can be advantageous for Machine Learning in following three ways. It can be a fast function approximator, the whole network is not used during inference, only a window of weights are used for evaluating a given input data point. It can be made to execute faster with Specialized high speed hardware like FPGAs. 
 
Advantages of CMAC are, it can support fast learning because of less complex calculations. It supports quick training without affecting memory structure. 

Limitations of CMAC for Machine Learning are that the approximate functions may not be smooth, the output might be in the form of step functions. Limited storage is also an issue with CMAC. 

\hfill

\textbf{5. Describe a form of Machine Learning discussed in class that uses no weights, generate a pseudo code and explain how it works.}

\textbf{Answer:} The Machine Learning algorithm that requires no weights is KNN(K- Nearest Neighbors).  It uses Shepard’s method - Inverse distance weighting, for classifying the data. General information about the significance of K is that, choosing a large K value leads to High bias and low variance, this is a over-fitting case as there will be large number of clusters to represent the data, each cluster wrapped closely around it. By contrast, choosing small K value leads to Low bias but High variance, under-fitting case, as there will be fewer clusters to represent the data, each cluster will have a large number of varied data points, thus creating a larger variance. The best K values controls a balance between over-fitting and under-fitting.

Algorithm explanation:
We are given a set of feature points $X$, which we want to cluster it into $K$ classes. We start by creating a Dictionary D with $K$ keys, each key will contain a list of values which will be obtained from the feature points based on the method which will be described next. We also have a list of Means $M$ of size $K$ containing the means of each cluster. Initially, the values of each means $m_{i}$ are assigned randomly. Next, we loop through each feature point and computes its distance form each of the means, the nearest distance $d$ is assigned to the respective class in the dictionary. The distance between a given feature point and mean can be calculated with Euclidean or Manhattan distance. The mean of each classes in the dictionary is computed and updated to the respective mean $m_{i}$. The above described procedure can be iterated for any number of time steps $T$ to obtain a closer fit to the data, or it can be stopped when the means $m_{i}$ are not changing with every iteration. The outline of the pseudo code is shown in Algorithm \ref{alg:knn}. The $ComputeDistance$ function in the pseudo code uses Euclidean distance.

\begin{algorithm}
	\caption{Pseudo code of $K$ Nearest Neighbor}
	\label{alg:knn}
	\begin{algorithmic}[1]
		\Function{ComputeDistance}{a,b}
			\State d = $\sqrt{(a-b)^{2}}$
			\State \Return d
		\EndFunction
		
		\State $X$ = List of Feature Points
		\State D = Dictionary of classes \Comment{Stores datapoints belonging to K Classes.}
		\State $M$ = $\{m_{1}, m_{2}, ..., m_{k}\}$ \Comment{Means of each class in $K$}
		\For{t = 1 to T}
			\State Clear contents of D
			\For{$x \in X$} \Comment{Loop through all points in $X$}
				\State Set $nearestDistance$ = $\infty$
				\State Set $class$ = None
				\For{$m_{i} \in M$} 
					\State Set $d$ = ComputeDistance($x$,$m_{i}$)
					\If{$d<nearestPoint$}
						\State Set $class = i$
						\State Set $nearestDistance = d$
					\EndIf
				\EndFor
			\State Store $x \rightarrow$ D[$class$]
			\EndFor
			
			\For{i=1 to K}
			\State $m_{i}$ = ComputeMean(D[$i$])
			\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}


\hfill

\textbf{6. Identify and explain differences between Forward and Inverse models in robot control systems.}

\textbf{Answer:} Control of a robot can be done in two ways. For example, let's take the case of a robotic arm with many joints. The forward control method involves commanding the individual joint velocities to move the end effector to a desired location. With forward control model, the future states of the robot can be obtained by evaluating the joint variable around the current neighborhood, with this feature, the robot can be controlled more robustly. 

However, in the case of Inverse control method, the desired end effector movement is commanded to the robot, subsequently, the individual joint rotations are calculated. With the inverse Jacobian method, the movement of the joints may not be deterministic, each joint will move in a direction that has the least cost of motion. Deadbeat control is also a form of inverse control, which involves computing the inverse of the forward function. Deadbeat control can be done by mapping the input and the forward functions's output. When the robot is performing a task, the input to the motors is located from the looked up table that was created. If the lookup value is missing, then the closest match will give an approximation of the function.

\hfill

\textbf{7. What was demonstrated by Braitenberg with his Vehicles, and explain why this was significant in 3 different  perspectives.}

\textbf{Answer:} 
Braitenberg showed that simple behaviors can be merged to create an incrementally complex set of behaviors. Three different behaviors are demonstrated and the control mapping of motors are visualized. He demonstrates the idea with a car like robot fitted with 2 wheels(actuators) 2 proximity sensors on either sides. In Vehicle 1, Movement speed is proportional to the intensity of light, in darkness, the vehicle stops. In Vehicle 2a, more light is shown on the right side of the vehicle, subsequently, car turns to the left, as the right wheels spin faster. In Vehicle 2b, the light is exposed on the left side, making the car turn to the right side. This shows that simple and straightforward logic is able to form this behavior similar to a moth quickly evading a predator such as a bat.

\hfill

\textbf{8. Identify 3 additional influential pioneers in the development of Behavior-Based Robotics discussed in class, and explain how their approaches and methodologies differed.}

\textbf{Answer:} 
Brooks, Arkin and Balch worked on the idea of subsumption architecture, Motor schema and Temporal sequencing respectively, as a part of Behavior based robotics. Additionally there are plenty of other approaches taken in Behavior based robotics such as Active perception, Deictic representation, and Expectations which will be discussed below.

Active perception is more towards the act of perceiving than acting on it. With that said, however, we can leverage this idea and act with the motor system based on the perception inference. During 1994, Wilkes and Tsotsos at University of Toronto controls a video camera on and defines three behaviors for controlling it. "Image line centering," orient object vertically using rotation control. "Image line following," centering of camera as the object is moved parallel to image plane, using tangential control.And "Camera distance correction," providing translation movement of object along the camera's optical axis, using translation control. These behaviors gives the ability for the camera to move in space and aids in exploration and confirms any priors of it's environment.

During 1994, at university of Virginia, Brill conducts a research based on deictic representaions to include local spatial memory. The idea is that, the robot can both react based on the current perceived data, as well as make a decision based on the reminiscent memory. 

The role of expectations in a behavior can limit the computational complexity and help algorithms run in real time. Expectations can tell the perception system where to look for and how the event will appear. Around 1984, at the University of Maryland, Waxman et al. performed recognition and following of roadways with an autonomous vehicle. The first step involved is to identify the road without knowledge of the vehicle's position. After locating the road, the inertial sensors are used to estimation the relative movement of the road surface. This belief is used as a location prior for the vision algorithm to look for the road in the appropriate location in the image. This reduces the computation cost of evaluating the whole image.

Answer for this question is referred from the book "Behavior Based Robotics "\cite{arkin}, Chapter 7.


\hfill

\textbf{9. Give 5 concrete examples on how evolutionary computation can be used to develop useful robot behaviors.}

\textbf{Answer:} Evolutionary computations are combination/mutations based algorithms. A model is evaluated at each scene with a strategically chosen combination of parameters. The new combination is made based on the previous performances of the said parameters. Evolutionary algorithms can be classified into five categories: Genetic Algorithm, Genetic program, Evolutionary strategies, Evolutionary program and Learning classifier program. The robots itself can evolve in terms of Morphology, Behavior and the Hardware it runs on. The idea of evolution in robots is that, such an approach will eventually allow robot developers to obtain higher quality behavior than through direct coding.

Walking has been one of the most researched areas in robotics, the goal is to attain a robust controller energy efficient gaits based on the terrain. Gomi and Ide (1998) evolved the gaits of an eight-legged robot using genotypes made of eight similarly organized sets of genes, each gene coding for leg motion characteristics, such as the amount of delay after which the leg begins to move, the direction of the leg’s motion, the end positions of both vertical and horizontal swings of the leg, and the vertical and horizontal angular speeds of the leg. After a few dozen generations, in which evaluation was performed on the robot, a mixture of tetrapod and wave gaits were obtained. Jakobi (1998) successfully evolved modular controllers based on Beer’s continuous recurrent  networks to control the same eight-legged robot as it engaged in walking about its environment avoiding obstacles and seeking out goals. The robot could smoothly change gait, move backward and forward, and even turn on the spot. 

An example of Evolution in Active vision is of a shape detection system. Evolved networks started by searching for the edge of the road and tracked its relative position with respect to the edge of the windshield in order to control steering and acceleration. This behavior was supported by the development of sensitivity to oriented edges, as in the previous experiments. Furthermore, the results showed that the development of visual receptive ﬁelds was signiﬁcantly and consistently affected by active vision as compared to the development of re-ceptive ﬁelds passively exposed to the same set of sample images. In other words, robots evolved with active vision developed sensitivity to a smaller subset of features in the environment and actively tracked those features to maintain a stable behavior. dicate that behavioral systems that are free to explore the environment can solve visually mediated tasks with
much simpler architectures and computational resources than those typically advocated in computer vision. This is possible because these systems rely on behavior to self-select the visual stimulation that is most useful for the task to be performed and that matches their computational properties.

Experiments on Computational Neuro-Ethology based Evolution are performed by correlating the robot position and orientation with the activation of the internal neurons in real time while an evolved individual freely moved in the environment, it was found that some neurons specialized in reactive behaviors, such as obstacle avoidance and forward motion. Other neurons instead displayed more complex activation patterns. One of them revealed a pattern of activation levels that depended on whether the robot was oriented facing the light tower or facing the opposite direction toward the recharging area never crossed the two “gate walls” visible in the activation maps around the recharging station. When the robot faced the opposite direction, the same neuron displayed a gradient ﬁeld orthogonally aligned with the recharging area. This gradient provides an indication of the distance from the recharging area. Interestingly, this pattern of activity is not significantly affected by the charge level of the battery.

Evolution of Learning explores the possibility of genetically encoding and evolving the 'learning rules' associated to the different synaptic connections of a neural network embedded in a real robot. When compared to genetically determined neural controllers without
synaptic plasticity, the evolution of learning rules was faster and produced significantly better fitness values in all situations. For example, genetically determined individuals (i.e., individuals whose genes encode the synaptic strengths, but not the learning rules) solved the light-switching task by turning in circles tuned to the dimensions of the evolutionary arena which eventually take them over the light switch area and then over the light bulb area. Instead, evolved adaptive individuals modiﬁed their synapses so as to develop in sequence the following set of behaviors: (a) avoid walls; (b) locate and go toward light switch; (c) locate and go toward light bulb; (d) stay under light bulb if it is on. Most importantly, these robots displayed remarkable adaptive properties after evolution. Best evolved individuals that were transferred perfectly from simulated to physical robots accomplished the task when the light and reﬂection properties of the environment were modiﬁed. They accomplished the task when light bulb and light switch were positioned at different locations, and transferred well across morphologically different robotic platforms. In other words, these robots evolved the ability to solve a partially unknown problem by adapting on the ﬂy, rather than a solution to a speciﬁc problem seen during evolution.



In the evolution of self-teaching controllers, evolving robots should develop an ability to understand whether they are currently located in an environment with white or black walls and modify their behavior accordingly.
Analysis of evolved robots revealed that they developed two different behaviors that were adapted to the particular arena where they happened to be “born.” Evolving robots did not inherit an ability to behave effectively, but rather a predisposition to learn to behave. This predisposition to learn involved several aspects such as a tendency to move so as to experience useful learning experiences and a tendency to acquire useful adaptive characters through learning.

The topic "Evolution of Behavioral Systems" from Chapter 6 of the book Bio-inspired artificial intelligence: theories, methods, and technologies \cite{floreano} has helped me answer this question.


\hfill 

\textbf{10. Describe in detail the use of shaping for creating a behavior-based robot controller.}

\textbf{Answer:} Shaping can be used to help the robot learn new behaviors through reinforcement. The types of behaviors that can be shaped are: Topography - 	the form of behavior, Frequency - number of responses obtained per unit time, Latency - how long after the stimulus is the behavior observed , Duration of how long the behavior lasts, and Amplitude. Shaping is useful for creating a makes the robot learning to be combined with other procedures such as chaining and imitation. With chaining, the behavioral collection of a robot can be gradually expanded \cite{johnApplied}. Ultimately, shaping is used as a method of programming robots to progress through initial, intermediate, and final goals to achieve more complicated sets of commands. Shaping a behavior can be time-consuming, it is an important tactic for teaching new behaviors. Shaping is useful when it is unlikely that novel target behaviors will be learned by following instructions, gaining incidental exposure, or providing models, physical cues, or verbal prompts. There are two elements in shaping: differential reinforcement and successive approximations. With differential reinforcement, the feedback to the robot system is provided by the reinforce, rather than the feedback being supplied by stimuli. By contrast, successive approximations is when the feedback is provided by the stimuli to the robot, this will be discussed in the next paragraph. Behavior chaining procedure is one procedure where shaping can be simplified, chaining is where a linked sequence of responses leads to a terminal outcome. Instead of trying to teach the complete behavior at  once, it is better to split the global behavior into components, to train each component separately, and then to train the agent to correctly coordinate the component behaviors. This procedure is reminiscent of the shaping procedure that psychologists use in their laboratories when they train an animal to produce a predefined response.


With a process of successive approximations in shaping, a new behavior that initially is not in the person’s repertoire is developed by reinforcing responses that meet a gradually changing criterion, toward the terminal behavior. Additionally, the changing response
criteria employed in shaping are topographical in nature, requiring different forms of behavior at each new level. The multiple probe design is more appropriate for analyzing a shaping program because each new response criterion (successive approximation) represents a different response class whose frequency of occurrence is not wholly dependent on the frequency of behaviors meeting other criteria in the shaping program. Conversely, the changing criterion design is best suited for evaluating the effects of instructional techniques on stepwise changes in the rate, accuracy, duration, or latency of a single target behavior.

Another form of shaping that can be done is Stimulus control which can be accomplished with stimulus fading and stimulus shape transformations. The previous methods, focused on response prompts that do not change the task stimuli or materials. The stimulus control shaping procedures modify the task stimuli or materials gradually and systematically to prompt correct responses. This form of shaping can be performed which helps the robot to learn about the physical quantities of the environment like light, heat and sensory details. 

A robot controller can also be developed based on "Imitative" shaping. Imitative repertoire enables the quick acquisition of new, complex behaviors characteristic of many human endeavors. Typically developing children acquire many skills by imitating unplanned models. Most children imitate the behavior of others without undue use of prompts or contrived reinforcement arrangements by parents, teachers, or other caregivers. For example, it is reported that infants as young as 12 to 21 days old can imitate adult hand and facial models. This shows that by using the same observation, a robot can also be trained in a similar fashion to control its actuators.



\begin{thebibliography}{9}
	\bibitem{localglobal}
	(2008) Global Learning vs. Local Learning. In: Machine Learning. Advanced Topics in Science and Technology in China. Springer, Berlin, Heidelberg. 
	
	\bibitem{svmCMAC}
	G. Horvath, "CMAC neural network as an SVM with B-spline kernel functions," Proceedings of the 20th IEEE Instrumentation Technology Conference (Cat. No.03CH37412), 2003, pp. 1108-1113 vol.2, doi: 10.1109/IMTC.2003.1207926.

	\bibitem{kernelCMAC}
	Horváth, Gábor. "Kernel CMAC: an efficient neural network for classification and regression." Acta Polytechnica Hungarica 3.1 (2006): 5-20.
	
	\bibitem{qCMAC}
	Yuan-Pao Hsu, Wei-Cheng Jiang and Hsin-Yi Lin, "A CMAC-Q-Learning based Dyna agent," 2008 SICE Annual Conference, 2008, pp. 2946-2950, doi: 10.1109/SICE.2008.4655167.
	
	\bibitem{reinforcemetCMAC}
	Tham, Chen K. "Reinforcement learning of multiple tasks using a hierarchical CMAC architecture." Robotics and Autonomous Systems 15.4 (1995): 247-274.
	
	\bibitem{arkin}
	Arkin, Ronald C., and Ronald C. Arkin. Behavior-based robotics. MIT press, 1998.
	
	\bibitem{floreano}
	Floreano, Dario, and Claudio Mattiussi. Bio-inspired artificial intelligence: theories, methods, and technologies. MIT press, 2008.
	
	\bibitem{johnApplied}
	Cooper, John O, Timothy E. Heron, and William L. Heward. Applied Behavior Analysis. Columbus: Merrill Pub. Co, 1987. Print.
	
\end{thebibliography}


\end{document}

